{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "import os\n",
    "module_path = os.path.abspath(os.path.join('/data/jiawei_li/doubtbot')) \n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from data import load_data, transform_to_question_answer_pair\n",
    "from eval import run_critic_eval\n",
    "from model_wrappers import Llama2Wrapper, Llama3Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = load_data()\n",
    "sample = transform_to_question_answer_pair(train_data[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5e4047551f44c16909cb089369047aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# critic = Llama2Wrapper(\"llama2_7b\", \"meta-llama/Llama-2-7b-chat-hf\")\n",
    "critic = judge = Llama3Wrapper(\n",
    "    \"llama3_8b\", \"/data/public_models/huggingface/meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n",
      " 10%|█         | 1/10 [00:09<01:28,  9.82s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 20%|██        | 2/10 [00:17<01:07,  8.46s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 30%|███       | 3/10 [00:29<01:12, 10.35s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 40%|████      | 4/10 [00:39<00:59,  9.89s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 50%|█████     | 5/10 [00:46<00:44,  8.93s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 60%|██████    | 6/10 [00:57<00:39,  9.82s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 70%|███████   | 7/10 [01:09<00:31, 10.45s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 80%|████████  | 8/10 [01:18<00:20, 10.02s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 90%|█████████ | 9/10 [01:26<00:09,  9.23s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "100%|██████████| 10/10 [01:38<00:00,  9.85s/it]\n"
     ]
    }
   ],
   "source": [
    "run_critic_eval(critic, judge, train_data[:10], \"/data/jiawei_li/doubtbot/data/results.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def print_entry(file_path, i=0):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "        if i < 0 or i >= len(data):\n",
    "            print(f\"Index {i} is out of range for the JSON data.\")\n",
    "            return\n",
    "        ith_entry = data[i]\n",
    "        json_str = json.dumps(ith_entry, indent=4)\n",
    "        for line in json_str.split('\\n'):\n",
    "            print(line)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_confidence_pairs(json_file_path):\n",
    "    with open(json_file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    confidence_pairs = []\n",
    "    \n",
    "    for entry in data:\n",
    "        pre_critique = entry.get(\"judge_confidence_pre_critique\")\n",
    "        post_critique = entry.get(\"judge_confidence_post_critique\")\n",
    "        \n",
    "        if pre_critique is not None and post_critique is not None:\n",
    "            confidence_pairs.append((pre_critique, post_critique))\n",
    "    \n",
    "    return confidence_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.9956326381262863, 0.2341445588135122),\n",
       " (0.9953804753874083, 0.15949336179340529),\n",
       " (0.9959349617750168, 0.19929126861241817),\n",
       " (0.9983626464931707, 0.6756634646890323),\n",
       " (0.9961813111568498, 0.26031967462862315),\n",
       " (0.9937442624489288, 0.06039730818017165),\n",
       " (0.997176404682894, 0.12338046930955895),\n",
       " (0.9943462282442064, 0.3011536833443787),\n",
       " (0.9966523141244802, 0.15110926102105712),\n",
       " (0.9989210131260612, 0.19367731820795145)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_confidence_pairs(\"/data/jiawei_li/doubtbot/data/results.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"question_answer_pair\": {\n",
      "        \"question\": \"Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn?\",\n",
      "        \"answer_correct\": {\n",
      "            \"numeric\": 10.0,\n",
      "            \"proof\": \"Weng earns 12/60 = $0.2 per minute.\\nWorking 50 minutes, she earned 0.2 x 50 = $10.\\n#### 10\"\n",
      "        },\n",
      "        \"answer_incorrect\": {\n",
      "            \"numeric\": 50.2,\n",
      "            \"proof\": \"Weng earns 12/60 = $0.2 per minute.\\nWorking 50 minutes, she earned 0.2 + 50 = $50.2.\\n#### 50.2\"\n",
      "        }\n",
      "    },\n",
      "    \"critic\": \"llama3_8b\",\n",
      "    \"judge\": \"llama3_8b\",\n",
      "    \"critique\": \"The incorrect answer claims that Weng earns $0.2 per minute, which is calculated by dividing the hourly wage of $12 by 60 minutes. This is where the mistake begins. The problem asks how much Weng earned in 50 minutes, not how much she earns per minute.\\n\\nThe correct calculation would be to multiply the hourly wage by the number of hours Weng worked, which is 50 minutes divided by 60 (to convert minutes to hours). This gives us:\\n\\n50 minutes / 60 = 0.833 hours (approximately)\\n\\nNow, multiply the hourly wage by this number of hours:\\n\\n$12/hour x 0.833 hours = $10.00\\n\\nSo, Weng earned approximately $10.00 for the 50 minutes of babysitting. However, the incorrect answer claims she earned $10.00 based on an incorrect calculation, which is why the proof is invalid.\\n\\nIn summary, the mistake lies in the incorrect calculation of the hourly wage per minute, and the failure to convert the 50 minutes to hours before multiplying by the hourly wage. A correct solution would involve converting the minutes to hours and then multiplying by the hourly wage.\",\n",
      "    \"judge_confidence_pre_critique\": 0.9953804753874083,\n",
      "    \"judge_confidence_post_critique\": 0.15949336179340529\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print_entry(\"/data/jiawei_li/doubtbot/data/results.json\", 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
